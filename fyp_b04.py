# -*- coding: utf-8 -*-
"""FYP B04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XlYk4bsDgimbC_TWhoa9bTvosKGuAeb3
"""

from google.colab import drive
drive.mount('/content/drive')

from PIL import Image, ImageEnhance
import os
import numpy as np


def rotate_and_save_image(image_path, output_dir, degrees, folder_name, i):
    with Image.open(image_path) as img:
        rotated_img = img.rotate(degrees, expand=True)
        base_name = os.path.basename(image_path)
        file_name, ext = os.path.splitext(base_name)
        output_file_name = f"{folder_name}rotated{i}{ext}"
        output_path = os.path.join(output_dir, output_file_name)

        rotated_img = rotated_img.convert('RGB')

        rotated_img.save(output_path)
        print(f"Saved rotated image to {output_path}")
        return i + 1

def process_images(image_dir, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    folder_name = os.path.basename(image_dir)
    i = 1
    for file_name in os.listdir(image_dir):
        if file_name.endswith(('.jpg', '.png', '.jpeg')):
            image_path = os.path.join(image_dir, file_name)
            for degrees in [90, 180, 270, 360]:
                i = rotate_and_save_image(image_path, output_dir, degrees, folder_name, i)
    return i # Return the final value of i

def adjust_contrast_and_save_image(image_path, output_dir, contrast_factor, folder_name, i):
    with Image.open(image_path) as img:
        # Ensure the image is in RGB mode
        if img.mode != 'RGB':
            img = img.convert('RGB')

        # Create a contrast enhancer
        enhancer = ImageEnhance.Contrast(img)

        # Adjust the contrast
        adjusted_img = enhancer.enhance(contrast_factor)

        # Prepare the output file name
        base_name = os.path.basename(image_path)
        file_name, ext = os.path.splitext(base_name)
        output_file_name = f"{folder_name}contrast{i}{ext}"
        output_path = os.path.join(output_dir, output_file_name)

        # Save the adjusted image
        adjusted_img.save(output_path)
        print(f"Saved adjusted image to {output_path}")
        return i + 1

def process_images_with_contrast(image_dir, output_dir, x):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    folder_name = os.path.basename(image_dir)
    i = 1
    for file_name in os.listdir(image_dir):
        if file_name.endswith(('.jpg', '.png', '.jpeg')):
            image_path = os.path.join(image_dir, file_name)
            for contrast_factor in [0.5, 1.5]:
                i = adjust_contrast_and_save_image(image_path, output_dir, contrast_factor, folder_name, i)

def resize_and_save_image(image_path, output_dir, new_size=(380, 380), folder_name=None, i=1):
    with Image.open(image_path) as img:
        # Ensure the image is in RGB mode
        if img.mode != 'RGB':
            img = img.convert('RGB')

        # Resize the image
        resized_img = img.resize(new_size) # Defaults to Image.BICUBIC

        # Prepare the output file name
        base_name = os.path.basename(image_path)
        file_name, ext = os.path.splitext(base_name)
        output_file_name = f"{folder_name}_{i}{ext}"
        output_path = os.path.join(output_dir, output_file_name)

        # Save the resized image
        resized_img.save(output_path)
        print(f"Saved resized image to {output_path}")
        return i + 1

def process_images_with_resize(image_dir, output_dir, new_size=(380, 380)):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    folder_name = os.path.basename(image_dir)
    i = 1
    for file_name in os.listdir(image_dir):
        if file_name.endswith(('.jpg', '.png', '.jpeg')):
            image_path = os.path.join(image_dir, file_name)
            i = resize_and_save_image(image_path, output_dir, new_size, folder_name, i)
    return i # Return the final value of i

def add_gaussian_noise(image_dir, output_dir, sigma_values, start_index=1):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    i = start_index
    for file_name in os.listdir(image_dir):
        if file_name.endswith(('.jpg', '.png', '.jpeg')):
            image_path = os.path.join(image_dir, file_name)
            try:
                with Image.open(image_path) as img:
                    img_array = np.array(img)
                    for sigma in sigma_values:
                        noise = np.random.normal(0, sigma, img_array.shape)
                        noisy_img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)
                        noisy_img = Image.fromarray(noisy_img_array)

                        base_name, ext = os.path.splitext(file_name)
                        output_file_name = f"{base_name}noisy_sigma{sigma}{ext}"
                        output_path = os.path.join(output_dir, output_file_name)

                        noisy_img.save(output_path)
                        print(f"Saved noisy image with sigma {sigma} to {output_path}")
                        i += 1
            except Exception as e:
                print(f"Error adding Gaussian noise to {image_path}: {e}")
    return i

#for folder_index in range(1, 7):
#image_dir = f'/content/drive/MyDrive/preprocessing/Input/grade1'
#output_dir = f'/content/drive/MyDrive/preprocessing/Buffer/Grade (1)'
output_dir1 = f'/content/drive/MyDrive/preprocessing/14,400_Output/Grade (5)'
image_dir1 = f'/content/drive/MyDrive/preprocessing/Buffer/Grade (5)'
#sigma_values = [20, 30, 40]

    # Process images and get the final value of i
#final_i = process_images(image_dir, output_dir)

    # Use the final value of i as the starting point for process_images_with_contrast
#process_images_with_contrast(image_dir1, output_dir, final_i)

#final_index = add_gaussian_noise(image_dir1, output_dir, sigma_values)

    # Now, process images with resize
final_i_resize = process_images_with_resize(image_dir1, output_dir1)

import os
import glob

# Define the directory path
directory_path = '/content/drive/MyDrive/preprocessing/Buffer/Grade (5)'

# Define the pattern to match the file names
pattern = os.path.join(directory_path,'*contrast*noisy_sigma*')

# Find all files matching the pattern
files_to_delete = glob.glob(pattern)

# Count the files
file_count = len(files_to_delete)
print(f"Total files to delete: {file_count}")

# Prompt the user for confirmation
user_input = input("Are you sure you want to delete these files? (y/n): ")

# Proceed with deletion if the user confirms
if user_input.lower() == 'y':
    # Iterate over the files and delete them
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            print(f"Deleted: {file_path}")
        except Exception as e:
            print(f"Error deleting {file_path}: {e}")
else:
    print("Operation cancelled.")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import EfficientNetB4  # Change here
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

# Load the pre-trained EfficientNetB4 model
base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(380, 380, 3))  # Change input_shape

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Create the model
model = Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(6, activation='softmax'))  # 6 output classes

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Load your data and preprocess it
dataset_path = os.listdir('/content/drive/MyDrive/preprocessing/14,400_Output')
print (dataset_path)  #the classes in this dataset are the file names
print("Types of classes labels found: ", len(dataset_path))

class_labels = []

for item in dataset_path:
 # Get all the file names
 all_classes = os.listdir('/content/drive/MyDrive/preprocessing/14,400_Output' + '/' +item)
 #print(all_classes)

 # Add them to the list
 for room in all_classes:
    class_labels.append((item, str('dataset_path' + '/' +item) + '/' + room))

# Build a dataframe
df = pd.DataFrame(data=class_labels, columns=['Labels', 'image'])
print(df.head())
print(df.tail())

print("Total number of images in the dataset: ", len(df))

label_count = df['Labels'].value_counts()
print(label_count)



import cv2
import os
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
image = []
labels = []
path = '/content/drive/MyDrive/preprocessing/14,400_Output/'
dataset_path = os.listdir(path)

# Determine the shape and data type of your images
# This is just an example; adjust according to your actual image dimensions and data type
image_shape = (380, 380, 3) # Example shape: 32x32 with 3 channels
dtype = np.uint8 # Example data type, adjust according to your images

# Create a memmap array for images
num_images = sum(len(os.listdir(os.path.join(path, i))) for i in dataset_path)
images_memmap = np.memmap('images_memmap.dat', dtype=dtype, mode='w+', shape=(num_images, *image_shape))

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Load images and labels, filling the memmap array
index = 0
for i in dataset_path:
    data_path = os.path.join(path, i)
    filenames = os.listdir(data_path)
    for f in filenames:
        img_path = os.path.join(data_path, f)
        img = cv2.imread(img_path)
        # Optionally resize the image here to reduce memory usage
        # img = cv2.resize(img, (new_width, new_height))
        images_memmap[index] = img
        labels.append(i)
        index += 1

# Encode labels
labels = label_encoder.fit_transform(labels)

# Convert lists to NumPy arrays for further processing
labels = np.array(labels)

import os
import cv2
import numpy as np
from tqdm import tqdm
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer

def image_generator(path, dataset_path, batch_size=4):
    images_batch = []
    labels_batch = []
    for i in tqdm(dataset_path):
        data_path = os.path.join(path, i)
        filenames = [f for f in os.listdir(data_path) if f.endswith(('.jpg', '.png', '.jpeg'))]

        for f in filenames:
            img_path = os.path.join(data_path, f)
            image = cv2.imread(img_path)
            image = image.astype('float32') / 255.0 # Normalize the image data
            images_batch.append(image)
            labels_batch.append(i)
            rem = len(f) % batch_size

            if rem == 0:
                yield np.array(images_batch), np.array(labels_batch)
                images_batch = []
                labels_batch = []

    if images_batch:
        yield np.array(images_batch), np.array(labels_batch)

# Initialize generator
path = '/content/drive/MyDrive/preprocessing/14,400_Output'
dataset_path = os.listdir('/content/drive/MyDrive/preprocessing/14,400_Output')
gen = image_generator(path, dataset_path, batch_size=4)

# Collect all images and labels
images = []
labels = []
for batch_images, batch_labels in gen:
    images.extend(batch_images)
    labels.extend(batch_labels)

yield np.array(images_batch), np.array(labels_batch)

images = np.array(images)
labels = np.array(labels)

# Use LabelEncoder and OneHotEncoder for labels
y = np.array(labels)
y = y.reshape(-1, 1)

from sklearn.compose import ColumnTransformer
ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')
Y = ct.fit_transform(y)

import cv2
path = '/content/drive/MyDrive/preprocessing/14,400_Output/'
dataset_path = os.listdir('/content/drive/MyDrive/preprocessing/14,400_Output')

images = []
labels = []

for i in dataset_path:
    data_path = path + str(i)
    filenames = [i for i in os.listdir(data_path) ]

    for f in filenames:
        img = cv2.imread(data_path + '/' + f)
        images.append(img)
        labels.append(i)

#This model takes input images of shape (224, 224, 3), and the input data should range [0, 255].

images = np.array(images)

images = images.astype('float32') / 255.0
images.shape

from sklearn.preprocessing import LabelEncoder , OneHotEncoder
y=df['Labels'].values
print(y)

y_labelencoder = LabelEncoder ()
y = y_labelencoder.fit_transform (y)
print (y)

print (y)

print(Y[5:])
print(Y.shape)

print(image.shape)
print(Y.shape)



images_memmap, labels = shuffle(images_memmap, labels, random_state=1)
train_x, test_x, train_y, test_y = train_test_split(images_memmap, labels, test_size=0.10, random_state=415)
# Convert to float32 and normalize
train_x = train_x.astype('float32') / 255.0
test_x = test_x.astype('float32') / 255.

# Inspect the shape of the training and testing data
print(train_x.shape)
print(train_y.shape)
print(test_x.shape)
print(test_y.shape)
print(train_y.dtype)

# Keep only the first 3 channels (RGB)
train_x = train_x[:, :, :, :3]
test_x = test_x[:, :, :, :3]


# Expand dimensions to include the color channels
train_x = np.expand_dims(train_x, axis=-1)
test_x = np.expand_dims(test_x, axis=-1)

# Convert labels to array and float32
train_y = train_y.toarray()
test_y = test_y.toarray()
train_y = train_y.astype('float32')
test_y = test_y.astype('float32')

print(train_y.dtype)

# Example usage
for train_x, test_x, train_y, test_y in shuffle_and_split_chunks(images_memmap, labels, chunk_size):
    # Process the chunks here, e.g., feed them into a model for training# Train the model
    model.fit(train_x, train_y, epochs=100, validation_data=(test_x, test_y), batch_size=32)
    pass

# Train the model
model.fit(train_x, train_y, epochs=100, validation_data=(test_x, test_y), batch_size=32)

# Evaluate the model
test_loss, test_acc = model.evaluate(test_x, test_y)
print('Test accuracy:', test_acc)

# Evaluate the model
predictions = model.predict(test_x)
accuracy = tf.keras.metrics.categorical_accuracy(test_y, predictions)

# Calculate the mean average accuracy
total_accuracy = accuracy
num_batches = 1  # Assuming a single batch

# Calculate the mean average accuracy
mean_average_accuracy = total_accuracy / num_batches
print(f"Mean average accuracy: {mean_average_accuracy:.4f}")

import matplotlib.pyplot as plt


def plot_hist(hist):
    plt.plot(hist.history["accuracy"])
    #plt.plot(hist.history["val_accuracy"])
    plt.title("model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.show()


plot_hist(hist)

from matplotlib.pyplot import imread
from matplotlib.pyplot import imshow
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.imagenet_utils import decode_predictions
from tensorflow.keras.applications.imagenet_utils import preprocess_input


img_path = '/content/drive/MyDrive/DFU/Graded/Grade 0/132.jpg'

#img = image.load_img(img_path, target_size=(224, 224))
#x = img.img_to_array(img)

img = cv2.imread(img_path)
img = cv2.resize(img, (380, 380))

x = np.expand_dims(img, axis=0)
x = preprocess_input(x)

print('Input image shape:', x.shape)

my_image = imread(img_path)
imshow(my_image)

from sklearn.metrics import accuracy_score

# Assuming you have your true labels (y_true) and predicted labels (y_pred)
# After training your model and making predictions

# Calculate accuracy for each sample
accuracies = []
for true_label, pred_label in zip(y_true, y_pred):
    accuracy = accuracy_score(true_label, pred_label)
    accuracies.append(accuracy)

# Calculate the mean average accuracy
mean_accuracy = sum(accuracies) / len(accuracies)
print(f"Mean Average Accuracy: {mean_accuracy:.4f}")

preds=model.predict(x)
preds     # probabilities for being in each of the 3 classes

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import EfficientNetB4
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Load the pre-trained EfficientNetB4 model
base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(380, 380, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Create the model
model = Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(6, activation='softmax'))  # 6 output classes

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Load your data and preprocess it
dataset_path = '/content/drive/MyDrive/preprocessing/14,400_Output'
class_labels = [item for item in os.listdir(dataset_path)]
print("Types of class labels found:", len(class_labels))

# Use ImageDataGenerator to load and preprocess the data
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    dataset_path,
    target_size=(380, 380),
    batch_size=32,
    class_mode='categorical',
    subset='training')

test_generator = test_datagen.flow_from_directory(
    dataset_path,
    target_size=(380, 380),
    batch_size=32,
    class_mode='categorical',
    subset='validation')

# Train the model
model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=10,
    validation_data=test_generator,
    validation_steps=len(test_generator))

# Evaluate the model
test_loss, test_acc = model.evaluate(test_generator)
print('Test accuracy:', test_acc)

for data, labels in test_generator:
    print(data.shape, labels.shape)
    break

# Recreate the test_generator
test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    dataset_path,
    target_size=(380, 380),
    batch_size=32,
    class_mode='categorical',
    subset='validation')

# Evaluate the model
test_loss, test_acc = model.evaluate(test_generator)
print('Test accuracy:', test_acc)

# Evaluate the model
predictions = model.predict(test_x)
accuracy = tf.keras.metrics.categorical_accuracy(test_y, predictions)

# Calculate the mean average accuracy
total_accuracy = accuracy
num_batches = 1  # Assuming a single batch

# Calculate the mean average accuracy
mean_average_accuracy = total_accuracy / num_batches
print(f"Mean average accuracy: {mean_average_accuracy:.4f}")

